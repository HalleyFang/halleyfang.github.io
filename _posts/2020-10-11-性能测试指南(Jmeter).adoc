:page-categories: [guide]
:page-tags: [参考指南]
:author: halley.fang

本文将从测试需求分析、测试准备、脚本开发与测试执行、测试结果分析与调优、性能测试报告五个阶段对性能测试过程进行阐述。

//more

## 测试需求分析

性能需求分析是整个性能测试工作开展的基础，如果连性能的需求都没弄清楚，后面的性能测试执行其实是没有任何意义的，而且性能需求分析做的好不好直接影响到性能测试的结果。在需求分析阶段，测试人员需要与项目相关的人员进行沟通，收集各种项目资料，对系统进行分析，建立性能测试业务和数据模型，并将其转化为可衡量的具体性能指标，确认测试的目标。

### 测试背景和目的
首先，要确定本次性能测试的需求是什么，或者性能测试的目的是什么？ 本文把性能测试按目的分以下几种：

. 新系统能力验证
+
比如，你们刚好开发了一个新系统，在上线前需要验证系统性能。这种情况比较简单，你可以有更多的自由选择测试环境、压力点和测试工具，测试策略上也比较灵活。并且如果性能测试结果没有明显的短板，也不需要进行调优。

. 客户有明确要求
+
这是一个好的结果，这说明客户对性能测试有一定的了解，知道他们需要的系统要达到一个什么样的标准。如：系统要求同时满足100用户登陆，平均每个用户登陆时间不能超过5秒。这个需求很明确，当然也不排除一些不懂装懂的用户，提一些不现实的要求。
+
不管怎么说，用户提要求了，这个比较容易，你可以对现系统做一次性能测试，至于，是通过优化系统还是增加硬件设备才能达到要求。就不是测试考虑的问题了。

. 找出系统性能瓶颈
+
这个需求的目的就很明确了，目的就是找出系统的性能瓶颈，进行调优或硬件扩容，所以性能测试的重点在系统的架构分析和业务分析上面。

. 稳定性验证（强度测试）
+
稳定性是系统的一个重要指标，因为系统一旦上线，就有可能会长期处在用户的访问状态，可能以前没发现的一些问题就会暴漏出来。比较典型的就是内存溢出，这种需求在测试策略上就要考虑性能测试的运行时长。

### 测试类型

根据不同的测试目的可以选择不同的测试类型，性能测试的几种测试类型如下

. 基准测试：在给系统施加较低压力时，查看系统的运行状况并记录相关数做为基础参考。
. 负载测试：是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等。
. 压力测试：压力测试是评估系统处于或超过预期负载时系统的运行情况，关注点在于系统在峰值负载或超出最大载荷情况下的处理能力。
. 稳定性测试：在给系统加载一定业务压力的情况下，使系统运行一段时间，以此检测系统是否稳定。
. 并发测试：测试多个用户同时访问同一个应用、同一个模块或者数据记录时是否存在死锁或者其他性能问题。

### 测试范围
性能测试点的选取

* 发生频率非常高的（例如：某邮箱核心业务系统中的登录、收发邮件等业务，它们在每天的业务总量中占到90%以上）

* 关键程度非常高的（产品经理认为绝对不能出现问题的，如登录等）

* 资源占用非常严重的（导致磁盘 I/O 非常大的，例如某个业务进行结果提交时需要向数十个表存取数据，或者一个查询提交请求时会检索出大量的数据记录）


### 测试模型
测试模型构建工作由性能测试实施人员完成，在需求分析的基础上，对调研收集到的相关资料与信息进行分析梳理，本阶段的产出物为，各个测试场景，以及场景中典型业务操作及所占比率。

例如：登录系统新增图书包含登录和新增图书两个操作，登录一次可以进行n次新增操作，则新增操作占比要更多，再结合实际业务的使用评估比例，该测试模型则为登录系统新增图书：登录10%，新增图书90%


### 测试指标
. 对指标的描述包括以下属性：

* 准确
+
如某系统必须在不超过 10 秒的响应时间内，处理 20 起登录任务。再如发邮件时间最大不超过 5 秒以及平均时间在 2 秒以内。

* 一致
+
用户和性能测试工程师对有关术语的理解要一致，如:并发用户数、在线用户数、注册用户数:

* 特定
+
性能测试的需求一定是有条件的。如：检查系统后台关键业务数据 10G、操作数据量为 `20K` ，1500 个用户、500 个并发用户运行的负载下，连续运行12小时过程中，业务操作是否满足性能需求。


. 一般性能需求描述示例：
+
```
* Web首页打开速度 5s 以下，Web登陆速度 15s 以下。
* 邮件服务支持 50 万个在线用户
* 计费话单成功率达到 99.999% 以上。
* 在100个并发用户的高峰期，邮箱的基本功能，处理能力至少达到 10QPS(TPS).
  QPS(TPS)--每秒钟请求/事物 数量
* 系统能在高于实际系统运行压力 1 倍的情况下，稳定的运行 12 小时。
* 这个系统能否支撑200万的VU（每天登录系统的人次）
  VU--Virtual user(虚拟用户)
```

### 测试策略
测试策略一般根据测试目的、测试范围和测试模型来制定，选择对应的测类和测试方案。

对于一个特定的业务系统，用户一般会分散在一天的各个时间段进行访问。在不同的时间段中，用户使用业务系统的频率不同，而系统的繁忙程度不同。在一些特定的条件下，可能出现短时间内用户集中访问某个业务系统的情况。例如对于公文处理子系统而言，可能就存在短时间内大量用户查看并办理某条公文的情况。 在进行性能测试时，应当使用“考虑最坏情况的原则”。也就是应当在用户使用业务系统最频繁、对系统造成最大压力的情况下对系统的功能进行测试，判断各功能和页面是否能够满足性能的要求，系统的响应时间是否过长。

另一方面，系统性能的验证必须做到“覆盖全面”。虽然系统中各个功能的使用频率并不相同，一些功能的使用频率相对于其他功能来说比较低，但是在进行性能测试和优化时，不能忽略这些功能，编制测试用例时也不能仅仅选择最常用功能。例如可能所有的用户都会访问我的通知列表，但是一般只有5%的用户会使用通过系统设置模块查找某个用户的信息；但是在测试时，我们并不能因为查看用户信息功能的使用频率相对较少，而忽略掉这项功能的测试。所以，这里进行系统性能测试时，对于不同业务，用户的访问比例应该做一个合理分配。

在测试策略上，我们还应该考虑，同一个系统在不同硬件环境下的性能表现。从而让系统满足需求的情况下，硬件配置也能达到一个最佳的状态。过份的增加硬件来满足需求也是一种浪费。再说增加硬件设备不是能解决所有性能问题的。

### 测试计划
根据项目的进度要求以及规模，来进行人力与时间的安排。对于大型的性能测试，项目前期的需求调研，环境的部署，工具的选购或开发，人员对测试工具的学习与使用，性能测试的后进行，后期数据的分析与调优，都需要合理安排人员。更有甚者需要专业的，系统工程师、数据库工程师、软件开发工程师、网络工程师以及性能测试工程师的共同参与配合完成。不是一个性能测试人员就可以全部搞定的。

## 测试准备
### 测试环境
这里的测试环境主要指的软件硬件环境和网络环境。

性能测试最好在一个独立的环境内进行，这样不会受到外界的干扰，能够保证测试的数据是独立有效的。如果现你对某个已经上线的网站进行压力测试，那么你得到的数据不是独立的，因为你在做压力测试的时候，其它散户也在访问系统。

软件环境：
这里的软件环境主要指项目运行的环境，比如采用什么样的操作系统、中间件、和数据库。

硬件环境：
这里的硬件环境除了主要包括主机内部部件，CPU 、内存、磁盘以及主板、网卡等，传输介质和路由器也应该考虑在内，

网络环境：
网络环境除了考虑测试机与被系统服务器在一个局域网中进行，还应该保证这个网络的独立性。如果在在性能测试的过程中，其它机子也在消耗着路由器资源。那么路由器也会影响到数据库的传输速度。

### 测试数据
在很多时候，我们是要准备测试数据的，例如系统不允许相同用户的重复登录，那么必须要生成合法的用户数据。有时要对系统进行查询测试，只有在系统有一定数据量进才能验证出系统的真实性能。一个数据库中有两条数据和有两千万条数据，同相一条查询操作，对系统造成的压力是完全不一样的。

系统所需数据的分析可以参考以下方式：

* 历史数据分析有助于数据量级的确定。从历史数据入手，找出高峰期数据量。

* 从其他相似或者相同系统入手，进行数据分析，找出高峰期数据量。

* 无历史或者相关系统可以参考的时候，就要对系统的性能数据进行估算，包含系统容量，并发数等数据，估算以后给相关人员进行评审或者修订以后，按照大家同意的性能指标进行测试。

测试数据最好和真实数据相同，如果能够获得真实系统运行3个月的数据，我们就可以在此基础上进行性能测试。

### 测试工具
本文只介绍 Jmeter

#### 安装 JDK
. 从 Java 官方网站（官方下载地址: https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html ）下载 JDK ，需要 JDK 8 或者更高版本
. 安装过程以及环境变量配置请参考官方说明

#### 安装 Jmeter
. 从 Apache 官网（官方下载地址： http://jmeter.apache.org/ ）下载 Jmeter ，例如： `apache-jmeter-4.0.zip`
. 解压下载的 zip 包
. 进入 `bin` 目录，运行 `jmeter.bat` 启动，启动成功界面如图所示
+
.启动成功
image::{{page.root}}/images/jmeter001.png[启动成功]

#### 安装 badboy(可选)
下载 `Badboy` （官方下载地址： http://www.badboy.com.au ） 进行安装

[[jpgc]]
#### Jmeter 安装 jpgc 插件(可选)
. 登录 https://jmeter-plugins.org/wiki/PerfMonAgent/ 下载 `ServerAgent-2.2.1.zip` ，解压可以直接运行

. 到 https://jmeter-plugins.org/downloads/old/ 下载 `JMeterPlugins-Standard-1.4.0.zip` 和 `JMeterPlugins-Extras-1.4.0.zip`

. 然后解压两个 zip 包，把 jar 文件拷贝到 `/lib/ext` 文件夹下, 重新启动

. 将监控服务器的 `serverAgent` 拷贝到需监测的服务器

[[nmon]]
#### 安装 nmon(可选)
使用 nmon 进行服务器监控，根据系统下载对应的软件包，需要在服务器上部署一下。在运行场景之前启动监听，在执行完成后停止监听并获取结果文件，使用分析工具生成监控报告
参考网站： https://www.cnblogs.com/wnfindbug/p/5719181.html

## 脚本开发与测试执行
[NOTE]
====
. 本文将以"新增图书"功能为测试示例，介绍性能测试的方法和过程，性能测试工具使用 Jmeter
. 本文未进行描述的 Jmeter 配置，有配置需求的请参考 Jmeter 官方配置说明
====

[[创建线程组]]
### 创建线程组
. 启动 jmeter ，创建线程组
右击 `TestPlan` ,选择 `Add->Treads(Users)->Thread Group`
+
.创建线程组
image::{{page.root}}/images/CreateThreadGroup.png[创建线程组]

. 设置一个 `Name` ，例如 `demo`
+
.创建线程组
image::{{page.root}}/images/demo001.png[创建线程组]

. 配置 `Thread Properties` 执行策略，参见<<set-thread-starting-strategy>>

### 创建请求消息
本文介绍代理录制、 badboy 录制、手动创建三种创建方式，实际测试中根据自身情况选择一种方式即可。

#### 代理录制请求消息
/////
. 创建HTTP Request Defaults，配置 `server ip` 和 `port`
/////

. 创建 `HTTP(S) Test Script Recorder`
+
.创建录制代理
image::{{page.root}}/images/demo002.png[创建录制代理]

. 配置 `port` 和 `Target Controller`
+
.创建录制代理
image::{{page.root}}/images/demo003.png[创建录制代理]

. 登录系统，进入图书界面,设置浏览器代理
+
[NOTE]
====
本文只测试新增图书这一简单场景，所以先进到界面后再设置代理，这样录制到的消息只是新增图书的消息。具体测试中以具体的功能业务灵活调整。
====
+
.图书界面
image::{{page.root}}/images/demo004.png[图书界面]
+
.浏览器代理设置
image::{{page.root}}/images/demo005.png[浏览器代理设置]

. 启动 `HTTP(S) Test Script Recorder`
点击start启动
+
.启动代理服务
image::{{page.root}}/images/demo006.png[启动代理服务]

. 浏览器操作新增图书动作，新增成功
+
.新增图书
image::{{page.root}}/images/demo007.png[新增图书]

. 停止 `HTTP(S) Test Script Recorder` ，查看录制的脚本（脚本根据实际情况调整消息请求，删除无用请求）
+
.录制脚本
image::{{page.root}}/images/demo008.png[录制脚本]

#### badboy 录制请求消息
通过 Jmeter 代理录制脚本后，会产生大量的无用的请求，尽管在代理中已经过滤了一部分图片或者 CSS、JS 文件。使用 badboy 录制可以提高开发效率。


.打开 Badboy
image::{{page.root}}/images/badboy001.png[打开Badboy]
. 在地址框输入要录入的 URL 地址，然后进行一些列你想要的操作
+
.Badboy 录制
image::{{page.root}}/images/badboy002.png[Badboy录制]
. 录制完成后，直接导出为 Jmeter 脚本
+
.Badboy 导出脚本
image::{{page.root}}/images/badboy003.png[Badboy导出脚本]
. Jmeter 打开录制导出的 `Jmeter` 脚本
+
.录制脚本
image::{{page.root}}/images/demo008.png[录制脚本]

#### 手动创建请求消息
手动创建即按照业务请求直接创建消息 `sampler` ，对工具不熟悉者且业务逻辑复杂的建议使用录制。手动创建可以通过浏览器开发者工具或者抓包工具查看请求消息和响应消息，用 Jmeter 相应的组件实现模拟请求。具体业务具体分析，以下步骤仅供参考：

. 添加 HTTP Request Defaults
. 添加 HTTP Header Manager
. 添加 HTTP Request
+
.手动创建
image::{{page.root}}/images/jmeter002.png[手动创建]

### 参数化
性能测试需要模拟多用户并发，请求与用户应该是一一对应，所以需要对请求消息内容进行参数化，从而实现请求与用户一一对应。
```json
"author__name": "${__RandomString(10,AaBbCcDdEeFfGgHhIiJjKkLlMmNn,teststring)}",
```

参数化方法有以下几种

. 用 Jmeter 中的函数获取参数值，`__Random`，`__threadNum`，`__CSVRead`，`__StringFromFile`，具体调用方法如下：
`${__Random(,,)}`，`$`，`${__CSVRead(,)}`，`${__StringFromFile(,,,)}`。
参看Jmeter函数的使用，通过菜单“选项”->“函数助手对话框”，即可在“函数助手”弹出框上找到Jmeter的函数。
其中 `${__Random(,,)}` 方法的第一个参数为随机数的下限，第二个参数为随机数的上限，第三个参数为储存随机数的变量名； `${__CSVRead(,)}` 方法中第一个参数是文件名，第二个参数是文件中的列（列数从0开始）； `${__StringFromFile(,,,)}` 方法中第一个参数是文件名，`${__StringFromFile(,,,)}` 方法中没有指定读取文件中的哪一列的参数，所以 `${__StringFromFile(,,,)}` 只能读取包含一列的文件。
. 用户定义的变量
.. 添加“配置元件”->“用户定义的变量”
.. “名称”中输入变量名称，此处以登录为例，定义两个变量 `*username*` 和 `*password*` 。“值”中可以直接输入值，也可以通过 Jmeter 的函数 `__CSVRead` ，`__StringFromFile` 从 `csv` 或 `dat` 文件中读取，还可以通过前缀加随机数的方法设置参数。
当参数值是某个前缀加一个数字时，可以用前缀名加 `${__Random(,,)}` 或 `$` 的方法设置参数值。如进行登录测试之前，先准备了用户名为 `perf_0` 到 `perf_1000` 的用户，那么用户名就可以设为 `perf_{__Random(0,1000,)}` 。
当参数值没有规律的且量不太大时，可以通过 `${__CSVRead(,)}` ， `${__StringFromFile(,,,)}`  从文件中读取，如将用户名和密码保存在 `user.csv` 文件中，`user.csv` 的内容如下：
+
```
oriana,123456
admin,admin
dandan,123456
```
+
因为 `*user.csv*` 文件中有两列数据，所以只能用 `${__CSVRead(,)}` 函数， `username` 参数后的值设为 `${__CSVRead(user.csv,0)}` ， `password` 参数后的值设为 `${__CSVRead(user.csv,1)}` 。
. 从 csv 文件中读取
当参数的值没有规律且量不太大时，可以用这种方法。
具体做法如下：
.. 创建一个 csv 文件，内容为参数的值集，每一个参数占一列，第一行就开始写参数值，不要写参数名
.. 在测试计划或线程组中添加一个“配置元件”-> `“CSV Data Set Config”`
.. `Filename` 中填写 csv 文件的完整路径（当 `csv` 文件在 `bin` 目录下时，只需给出文件名即可）
.. `Virable Names` 中填写变量名，如果 csv 文件中有多个变量，则用逗号隔开
. 从数据库中获取,当参数的值没有规律且量比较大时，可以选用这种方法,暂不做详细介绍。
. 用正则表达式从前面请求的响应数据中提取，具体操作见 Jmeter 官方文档正则表达式提取器

### 断言（可选）
断言用来判断响应是否符合预期，一般性能测试时不会使用，因为会消耗一定的性能影响测试结果，性能测试是否成功一般可以在执行完成后进行数据统计，计算成功率。本文不做详细介绍，有需要的同学请参考 Jmeter 官方文档

### 集合点（可选）
通过计时器 `Synchronizing Timer` 实现的假集合点功能, `Synchronizing Timer` 界面有两个参数设置，`Number of Simulated User to Group` 和 `Timeout in milliseconds`

. `Number of Simulated User to Group` ： 模拟用户到组数,即设置组的用户数，达到该用户数后才进行接口的请求
. `Timeout in milliseconds` : 超时（毫秒）,设置超时时间，即组在超时时间后达不到设置的线程数时，会丢弃继续请求

.集合点
image::{{page.root}}/images/demo013.png[集合点]

### 关联（可选）
关联有以下两种方法：

. 从前一个请求中取，用正则表达式提取器。
具体方法，在需要获得数据的请求上右击添加一个后置处理器-->正则表达式提取器
引用名称即下一个请求要引用的参数名称，如填写 `title` ，则可用 `${title}` 引用它。
正则表达式中()括起来的部分就是要提取的。.代表任意字符，*代表出现任意次。
模板，用 `$$` 引用起来，如果在正则表达式中有多个正则表达式（多个括号括起来的东东），则可以是 `$2$` ， `$3$` 等等，表示解析到的第几个值给 `title` 。
匹配数字，0代表随机，-1代表所有，其余正整数代表将在检查的内容中，第几个匹配的内容提取出来。

. 用 `xpath` 从前一个请求中取。这种形式比较适合于返回为 xml 片段的情况。
在需要获得数据的请求上右击添加一个后置处理器--> `xPath Extractor` 。
引用名称即下一个请求要引用的参数名称，如填写 `body` ，则可用 `${body}` 引用它。
`XPath query` ，即 `xpath` 的表达式，要符合 `xpath` 的语法。

### 设置思考时间（可选）
用户界面操作会有一定的操作时间，则性能测试时合理添加思考时间更能贴近实际使用场景，从而获得更可靠的性能数据

.思考时间
image::{{page.root}}/images/demo009.png[思考时间]

### 添加 Jmeter 监听
创建监听 `Aggregate Report，View Results Tree`

.监听
image::{{page.root}}/images/demo010.png[监听]

### 部署服务器监听
监听可以使用 jmeter 插件<<jpgc>> 或 <<nmon>>，使用 jmeter 则推荐使用插件，其他性能工具可以使用 `nmon`。使用 jpgc ，参考<<jpgc>>安装插件，使用步骤如下

. 添加 `jp@gc - PerfMon Metrics Collector`
+
.jp@gc监听
image::{{page.root}}/images/jmeter003.png[jp@gc监听]

. 配置监控的服务器以及监控内容
+
.jp@gc监听
image::{{page.root}}/images/jmeter004.png[jp@gc监听]

[[set-thread-starting-strategy]]
### 配置线程启动策略

. 线程数：虚拟用户数。一个虚拟用户占用一个进程或线程。 设置多少个虚拟用户数在这里也就是设置多少个线程数。
. 准备时长：设置的虚拟用户数需要多长时间全部启动。如果线程数为20，准备时长为10，那么需要10秒钟启动20个线程。也就是每秒钟启动2个线程。
. 循环次数：每个线程发送请求的次数。如果线程数为20， 循环次数为100，那么每个线程发送100次请求。总请求数为20*100=2000。如果勾选了 `永远` ,那么所有线程会一直发送请求，直到选择停止运行脚本。 `1S=1000MS,1MIN=60000MS,1H=3600000MS`

.启动策略
image::{{page.root}}/images/demo011.png[启动策略]

### 执行测试
. 运行线程

.运行线程
image::{{page.root}}/images/demo012.png[运行线程]

## 测试结果分析与调优

### 结果分析

指标说明请参考 <<KPI>>

. 查看结果树  `view results tree` 为了减少性能消耗该项建议勾选只显示错误消息，相关指标可以查看聚合报告或添加其他监听。
+
```
Thread Name: 线程组名称
Sample Start: 启动开始时间
Load time: 加载时长
Latency: 等待时长
Size in bytes: 发送的数据总大小 1GB＝1024MB，1MB＝1024KB，1KB＝1024Bytes Headers
size in bytes: 发送头大小
Body size in bytes: 发送数据的其余部分大小
Sample Count: 发送统计
Error Count: 交互错误统计
Response code: 返回码
Response message: 返回信息
Response headers: 返回的头部信息
```

. 聚合报告  `aggregate report`
+
```
Label：请求类型，对应在测试计划下填写的请求名称。
Samples：当前发送到服务器的请求总数，对应图形报表中的样本数目。
Average：平均响应时间，计算方法是总运行时间除以发送到服务器的总请求数，对应图形报表中的平均值。
Median：中位数,也就是50%用户的响应时间,即图形报表中的中间值。
90%Line：90%用户的响应时间
95%Line：95%用户的响应时间
99%Line：99%用户的响应时间
Min：服务器响应的最短时间
Max: 服务器响应的最长时间
Error%: 请求返回错误的百分比
Throughput: 服务器每单位时间处理的请求数，对应图形报表中的吞吐量。
KB/sec: 每秒钟请求的字节数。
```

### 性能瓶颈
性能测试调优需要先发现瓶颈，那么系统一般会存在哪些瓶颈：

. 硬件上的性能瓶颈：
+
一般指的是 CPU 、内存、磁盘 I/O 方面的问题，分为服务器硬件瓶颈、网络瓶颈（对局域网可以不考虑）、服务器操作系统瓶颈（参数配置）、中间件瓶颈（参数配置、数据库、 web 服务器等）、应用瓶颈（ SQL 语句、数据库设计、业务逻辑、算法等）。

. 应用软件上的性能瓶颈：
+
一般指的是应用服务器、 web 服务器等应用软件，还包括数据库系统。例如：中间件 weblogic 平台上配置的 JDBC 连接池的参数设置不合理，造成的瓶颈。

. 应用程序上的性能瓶颈：
+
一般指的是开发人员新开发出来的应用程序。例如，程序架构规划不合理，程序本身设计有问题（串行处理、请求的处理线程不够），造成系统在大量用户访问时性能低下而造成的瓶颈。

. 操作系统上的性能瓶颈：
+
一般指的是 windows、UNIX、Linux 等操作系统。例如，在进行性能测试，出现物理内存不足时，虚拟内存设置也不合理，虚拟内存的交换效率就会大大降低，从而导致行为的响应时间大大增加，这时认为操作系统上出现性能瓶颈。

. 网络设备上的性能瓶颈：
+
一般指的是防火墙、动态负载均衡器、交换机等设备。例如，在动态负载均衡器上设置了动态分发负载的机制，当发现某个应用服务器上的硬件资源已经到达极限时，动态负载均衡器将后续的交易请求发送到其他负载较轻的应用服务器上。在测试时发现，动态负载均衡器没有起到相应的作用，这时可以认为网络瓶颈。

性能测试出现的原因及其定位十分复杂，这里只是简单介绍常见的几种瓶颈类型和特征，而性能测试所需要做的就是根据各种情况因素综合考虑，然后协助开发人员，DBA ，运维人员一起定位性能瓶颈。

### 性能调优
. 确定问题

* 应用程序代码：在通常情况下，很多程序的性能问题都是写出来的，因此对于发现瓶颈的模块，应该首先检查一下代码。

* 数据库配置：经常引起整个系统运行缓慢，一些诸如 oracle 的大型数据库都是需要DBA进行正确的参数调整才能投产的。

* 操作系统配置：不合理就可能引起系统瓶颈。

* 硬件设置：硬盘速度、内存大小等都是容易引起瓶颈的原因，因此这些都是分析的重点。

* 网络：网络负载过重导致网络冲突和网络延迟。

. 分析问题
+
当确定了问题之后，我们要明确这个问题影响的是响应时间吞吐量，还是其他问题？是多数用户还是少数用户遇到了问题？如果是少数用户，这几个用户与其它用户的操作有什么不用？系统资源监控的结果是否正常？ CPU 的使用是否到达极限？ I/O  情况如何？问题是否集中在某一类模块中？ 是客户端还是服务器出现问题？ 系统硬件配置是否够用？实际负载是否超过了系统的负载能力？ 是否未对系统进行优化？
通过这些分析及一些与系统相关的问题，可以对系统瓶颈有更深入的了解，进而分析出真正的原因。

. 确定调整目标和解决方案
+
提高系统吞吐量，缩短响应时间，更好地支持并发。

. 测试解决方案
+
对通过解决方案调优后的系统进行基准测试。（基准测试是指通过设计科学的测试方法、测试工具和测试系统，实现对一类测试对象的某项性能指标进行定量的和可对比的测试）

. 分析调优结果
+
系统调优是否达到或者超出了预定目标？系统是整体性能得到了改善，还是以系统某部分性能来解决其他问题。调优是否可以结束了。

. 如果达到了预期目标，调优工作就基本可以结束了。如果未达标则重复以上步骤。

## 性能测试报告
参考 http://121.43.182.128:16080/pub/docs/output/ 性能测试报告-模板样例.doc

## 附件

[[KPI]]
### 性能指标说明

. 注册用户数
注册用户数指软件中已经注册的用户，这些用户是系统的潜在用户，随时都有可能上线。这个指标的意义在于让测试工程师了解系统数据中的数据总量和系统最大可能有多少用户同时在线。

. 在线用户数
在线用户数是指某一时刻已经登录系统的用户数量。在线用户数只是统计了登录系统的用户数量，这些用户不一定都对系统进行操作，对服务器产生压力。

. 并发用户数
不同于在线用户数，并发用户数是指某一时刻向服务器发送请求的在线用户数，他是衡量服务器并发容量和同步协调能力的重要指标，从这个含义上讲，我们可能会如下两种理解：
同一时刻向服务器发送相同或者不同请求的用户数，也就是说，既可以包括对某一业务的相同请求，也可以包括对多个业务的不同请求
同一时刻向服务器发送相同请求的用户数，仅限于某一业务的相同请求

. 请求的响应时间
响应时间就是用户感受软件系统为其服务所消耗的时间。对于 web 系统，请求的响应时间指的是从客户端发起的一个请求时间，到客户端接收到从服务器返回的响应结束。
. 在3秒之内，页面给予用户响应所有显示，可认为是很不错的
. 在3-5秒之内，页面给予用户响应所有显示，可认为是好的
. 在5-10秒之内，页面给予用户响应所有提示，可认为是勉强接收的
. 超过10秒后就有点让人不耐烦，用户很坑不会继续等待下去

. 事务的响应时间
事务是指用户在客户端做一种或多种业务所需要的操作集，事务的响应时间就是衡量用户执行这些操作集所花费的时间。在性能测试中，一般通过计算事务的开始时间和结束时间的差值来获取事务的响应时间。

. 每秒点击数
每秒点击数是指每秒钟像 web 服务器提交的 HTTP 请求数，它是衡量服务器处理能力的一个常用指标。需要注意的是，这里的响应时间并非鼠标的一次单击操作，因为在一次单击操作中，客户端可能向服务器发出多个 HTTP 请求，切勿混淆。

. 吞吐率
吞吐率通常指单位时间内从服务器返回的字节数，也可以单位时间内客户提交的请求数。吞吐率是大型web系统衡量自身负载能力的一个重要指标，一般来说，吞吐率越大，单位时间内处理的数据就越多，系统的负载能力也强。吞吐率与很多因素有关，服务器的硬件配置，网络的宽带及拓扑结构，软件的技术架构等。

. 业务成功率
指多用户对某一业务发起操作的成功率。例如，测试网络订票系统的并发处理性能，在早上8:00——8:30半小时的高峰里，要求能支持10万比订票业务，其中成功率不少于98%。也就是说系统允许200笔订票业务超时或者因其他原因导致未能订票成功。

. TPS
TPS 表示服务器每秒处理的事务数，他是衡量系统处理能力的一个非常重要的指标，在性能测试中，通过检测不同用户的 TPS ,可以估算出系统处理能力的拐点。

. 资源利用率
资源利用率就是指资源的使用情况，如 CPU 使用率、内存使用率、网络宽带的使用情况、磁盘 I/O 的输入输出量等系统硬件方面的监控指标。一个完整的系统是由软件和硬件组成，缺了任何一方都不可能成为一个正常运作的系统，所以资源利用率也是测试人员的一个监控点，并在当前软件的发展趋势下，硬件资源的成本也不可小视。

### nmon指标说明
https://blog.csdn.net/saraul/article/details/8570781
